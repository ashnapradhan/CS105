{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashnapradhan/CS105/blob/main/Final_Project_105.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5XFYIc_2kTQ"
      },
      "source": [
        "# **Israel/Palestine Text Mining : CS105 Final Project**\n",
        "Risa Onishi (3rd year data science)\n",
        "\n",
        "Sazen Shakya (3rd year computer science)\n",
        "\n",
        "Haru Sakamoto (3rd year data science)\n",
        "\n",
        "Ashna Pradhan (3rd year computer science)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1. Project Description**"
      ],
      "metadata": {
        "id": "C0JoXhCSiBbV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For our final project, we analyzed the public opinion on the Israel-Palestine War. Our goal was to gain a deeper understanding of the public’s opinion regarding the war and analyze what factors could have impacted the controversy and interactions in reddit comments. The basis of our analysis came from a dataset consisting of Reddit comments that were made on posts about the Israel-Palestine War. The dataset consists of data from 9/2/2023 (prior to the major events of 2023), and is updated on a daily basis to this day. The data that we used for this project consisted of the comment text, upvotes, downvotes, time created, subreddit, and controversiality. As the dataset is updated every day, extensive cleaning is required to use the data for analysis.\n"
      ],
      "metadata": {
        "id": "SfmEOov4iD9E"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkvmqGcpsy4s"
      },
      "source": [
        "#**2. Data Collection and Data Cleaning**\n",
        "**Dataset:** [Daily Public Opinion on Israel-Palestine War by Asaniczka\n",
        "](https://www.kaggle.com/datasets/asaniczka/reddit-on-israel-palestine-daily-updated?resource=download\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## a. Installations"
      ],
      "metadata": {
        "id": "Nn_hNgffv8b2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "qTINLLfFhRHR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6555141f-28f4-4576-b5d1-6544ac3eed3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.1.31)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.1)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.3)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.1.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.3.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=df5abdd8c54bb2b5629cfacd45ee0657a0c1daf0a0312445b578eab578c64cfd\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle\n",
        "!pip install scikit-learn pandas\n",
        "!pip install langdetect\n",
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Fo9MspWrLUE3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac2bdac6-839f-43de-bfd8-636b96cb00ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import ast\n",
        "from langdetect import detect\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from transformers import pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer, AutoConfig\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "import warnings\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "kw2CkI0cG1h2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b1939eb-c842-47f3-ffaa-4d7704e9e842"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "cp: cannot create regular file '/root/.kaggle/': Not a directory\n",
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/kaggle/cli.py\", line 68, in main\n",
            "    out = args.func(**command_args)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/kaggle/api/kaggle_api_extended.py\", line 1734, in dataset_download_cli\n",
            "    with self.build_kaggle_client() as kaggle:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/kaggle/api/kaggle_api_extended.py\", line 688, in build_kaggle_client\n",
            "    username=self.config_values['username'],\n",
            "             ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
            "KeyError: 'username'\n",
            "unzip:  cannot find or open /content/reddit-on-israel-palestine-daily-updated.zip, /content/reddit-on-israel-palestine-daily-updated.zip.zip or /content/reddit-on-israel-palestine-daily-updated.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "! cp /content/drive/MyDrive/Kaggle/kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download asaniczka/reddit-on-israel-palestine-daily-updated\n",
        "! unzip /content/reddit-on-israel-palestine-daily-updated.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NMoHg33aczA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "64f467da-372e-4860-cf44-0de06197cb95"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'reddit_opinion_PSE_ISR.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-d33042328e7c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moriginal_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'reddit_opinion_PSE_ISR.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'reddit_opinion_PSE_ISR.csv'"
          ]
        }
      ],
      "source": [
        "original_df = pd.read_csv('reddit_opinion_PSE_ISR.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AA3u27tw1n6",
        "outputId": "08989194-7582-438f-a859-43980d3d99fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWIL5XI_no_H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "600288b6-1c19-45dd-cace-116dabb866a8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'original_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-b1743af2103d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'self_text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'subreddit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'created_time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'controversiality'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ups'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'downs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sampled_data.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'original_df' is not defined"
          ]
        }
      ],
      "source": [
        "df = original_df.sample(n=10000, random_state=24)\n",
        "df = df[['self_text', 'subreddit', 'created_time', 'controversiality', 'ups', 'downs']]\n",
        "df.to_csv('sampled_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOQP_9Bfnv8r"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('sampled_data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syGsc1zdghgK"
      },
      "source": [
        "## b. Text Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4Jtrmn1m3rM"
      },
      "source": [
        "###i. Removing links and non-English"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "r_rnoK0eUGc0"
      },
      "outputs": [],
      "source": [
        "def detect_language(text):\n",
        "    try:\n",
        "        return detect(text)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def has_link(text):\n",
        "    link_pattern = re.compile(r'http\\S+|www\\S+')\n",
        "    return bool(link_pattern.search(text))\n",
        "\n",
        "df['language'] = df['self_text'].apply(detect_language)\n",
        "df = df[df['language'] == 'en']\n",
        "\n",
        "mask = df['self_text'].apply(has_link)\n",
        "df = df[~mask]\n",
        "\n",
        "df = df.drop(columns=['language'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Cleaning: Before starting any analysis on the dataset, we realized that there were many comments that were either in another language or had links. So we first cleaned the data by removing any comments that were not in English or contained a link (starting with “http” or “www”). We also got rid of any columns that we were not going to use, leaving us with the columns, self_text, subreddit, upvotes, downvotes, and controversiality.\n"
      ],
      "metadata": {
        "id": "N-J4JzAQL8eL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghgnStZMhwl_"
      },
      "source": [
        "###ii.  Tokenizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBeXJKEvn4fh"
      },
      "outputs": [],
      "source": [
        "custom_stopwords = {\"n't\", \"ve\", \"ca\", \"don\", \"d\", \"re\", \"m\", \"https\", \"gt\", \"s\", \"t\", \"u\", \" \",\n",
        "                    'yes', 'even', 'going', 'got', 'lot', 'probably', 'really', 'make', 'done', 'made',\n",
        "                    'many', 'give', 'done', 'two', 'sure', 'thing', 'things', \"'s\", \"'m\",\n",
        "                    'actually', 'would', 'like', 'also', 'though', 'may', 'go', 'literally', 'already',\n",
        "                    'much', 'could', 'something', 'back', 'let', 'know', 'maybe', 'think', 'saying', 'use',\n",
        "                    'yeah', 'keep', 'since', 'get', 'one', 'getting', 'lol', 'always', 'new', 'چپیه',\n",
        "                    'another', 'around', 'either', 'makes', 'try', 'see', 'trying', 'still', 'find', 'anyone',\n",
        "                    ',', '.', '\"', '“', '”' ,'\\'', '’', '(', ')', ':', '&', ';', 'put', 'part'}\n",
        "nltk_stopwords = set(stopwords.words('english'))\n",
        "all_stopwords = custom_stopwords.union(nltk_stopwords)\n",
        "\n",
        "def tokenize_text(text):\n",
        "    if isinstance(text, str):\n",
        "        words = text.split()\n",
        "        filtered_words = [word for word in words if len(word) <= 20]\n",
        "        filtered_text = ' '.join(filtered_words)\n",
        "        tokens = word_tokenize(filtered_text)\n",
        "        filtered_tokens = [\n",
        "            token for token in tokens\n",
        "            if token.lower() not in all_stopwords\n",
        "        ]\n",
        "        return filtered_tokens\n",
        "    else:\n",
        "        return ['']\n",
        "\n",
        "df['self_text_tokens'] = df['self_text'].apply(tokenize_text)\n",
        "all_tokens = \" \".join([\" \".join(tokens) for tokens in df['self_text_tokens']])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('sampled_data.csv', index=False)"
      ],
      "metadata": {
        "id": "k9ex0zak3Vo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used a set of stop words from the NLTK library as well as defined our own custom stopwords to create one large set of stopwords. We then tokenized the text and removed any stopwords."
      ],
      "metadata": {
        "id": "oDtx2saUqA4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Exploratory Data Analysis**"
      ],
      "metadata": {
        "id": "kHVJMmApwcDn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPo2P7Tgr_D5"
      },
      "source": [
        "## a. Word Cloud\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can begin our analysis by creating a word cloud of the dataset's top terms. The word cloud will show the most frequently occurring words in the dataset, with the size of each word reflecting its frequency of appearance. A word cloud can help us identify the most significant discussions of our topic and thus help drive our  investigation."
      ],
      "metadata": {
        "id": "gHuDd4O_CtpG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E59N1GeQm4GM"
      },
      "outputs": [],
      "source": [
        "wordcloud = WordCloud(\n",
        "    width=800,\n",
        "    height=400,\n",
        "    background_color='white',\n",
        "    colormap='plasma',\n",
        "    normalize_plurals=False,\n",
        "    stopwords=all_stopwords,\n",
        "    max_words=100\n",
        ").generate(all_tokens)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As depicted in our word cloud, terms such as \"Israel\", \"people\", and \"Hamas\" are the most frequently occurring terms of our dataset. The frequent appearance of terms like \"war,\" \"support,\" \"genocide,\" and \"Gaza\" suggest a focus on the violence and humanitarian aspects of the conflict. Additionally, the inclusion of terms such as \"right,\" \"state,\" \"history,\" and \"military\" point to discussions around national sovereignty, political rights, and military actions."
      ],
      "metadata": {
        "id": "vcnM0BvxCxMR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "#b. Bar Graphs"
      ],
      "metadata": {
        "id": "3Aum3vDS0Oa_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be using bar graphs to compare subreddits and see if there are any key terms or factors that contribute to a comment being controversial."
      ],
      "metadata": {
        "id": "N4fS3hIT1jui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subreddit_comments = df['subreddit'].value_counts()\n",
        "subreddit_comments = subreddit_comments.sort_values(ascending=False)\n",
        "\n",
        "top_subreddits = subreddit_comments.head(10)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(top_subreddits.index, top_subreddits.values)\n",
        "plt.title('Subreddits with Most Comments')\n",
        "plt.xlabel('Subreddit')\n",
        "plt.ylabel('Total Number of Comments')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c2ctV6ez2L7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "controversial_comments = df[(df['controversiality'] >= 0.98) & (df['controversiality'] <= 1)]\n",
        "subreddit_counts = controversial_comments['subreddit'].value_counts().head(10)\n",
        "top_subreddit = subreddit_counts.idxmax()\n",
        "max_count = subreddit_counts.max()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(subreddit_counts.index, subreddit_counts.values)\n",
        "plt.title('Subreddit with Most Highly Controversial Comments')\n",
        "plt.xlabel('Subreddit')\n",
        "plt.ylabel('Number of Controversial Comments')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Y2olk33b2Hap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first chart shows us the top 10 subreddits with the most comments. The second chart shows us the top 10 subreddits with the most controversial comments. The way we measured whether or not a comment was controversial was by gathering comments that had a controversiality rating of 0.98-1.00, which is a rating that is determined by the amount of likes and interactions a comment has. The overall comments takes the data from all 10000 datapoints, so has a high frequency. But since the number for comments that are controversial goes down drastically, the overall count also go down as well. As we can see, the subreddit with the most overall comments is IsrealPalestine subreddit, but the subreddit with the most controversial comments is worldnews. Although there are more comments in the IsrealPalestine subreddit, there is more controversy in the worldnew subreddit."
      ],
      "metadata": {
        "id": "y7iSikyI72g1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "highest_subreddit = controversial_comments['subreddit'].value_counts().idxmax()\n",
        "\n",
        "filtered_posts = controversial_comments[controversial_comments['subreddit'] == highest_subreddit]\n",
        "all_text = ' '.join(filtered_posts['self_text'].astype(str).tolist())\n",
        "\n",
        "tokens = word_tokenize(all_text)\n",
        "filtered_tokens = [w.lower() for w in tokens if w.isalpha() and w.lower() not in all_stopwords]\n",
        "word_counts = Counter(filtered_tokens)\n",
        "top_words = word_counts.most_common(10)\n",
        "\n",
        "words, counts = zip(*top_words)\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(x=words, y=counts)\n",
        "plt.title(f'Common Words in the Highest Controversial Subreddit: {highest_subreddit}')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8ZwpHz8j2Jq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "second_highest_subreddit = controversial_comments['subreddit'].value_counts().index[1]\n",
        "filtered_posts = controversial_comments[controversial_comments['subreddit'] == second_highest_subreddit]\n",
        "all_text = ' '.join(filtered_posts['self_text'].astype(str).tolist())\n",
        "\n",
        "tokens = word_tokenize(all_text)\n",
        "filtered_tokens = [w.lower() for w in tokens if w.isalpha() and w.lower() not in all_stopwords]\n",
        "word_counts = Counter(filtered_tokens)\n",
        "top_words = word_counts.most_common(10)\n",
        "\n",
        "words, counts = zip(*top_words)\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(x=words, y=counts)\n",
        "plt.title(f'Common Words in the 2nd Highest Controversial Subreddit: {second_highest_subreddit}')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YGbYEUkc4eWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following two graphs show the frequency of the most used words in the top 2 controversial subreddits, which are the 2 subreddits that had the most controversial comments. In the worldnews subreddit we see words that cover a broader scope, such as \"wars\", \"civilians\", \"countries\" and, \"country\". On the other hand, in the Israel-Palestine subreddit, we can see that there are more words that focus more on the conflict, such as \"palestinians\",\"palestinian\" \"jews\", and \"israeli\". We can also see that even though the total number of controversial comments is less for the IsraelPalestine subreddit, there is a way higher frequency of the word \"Israel\". From this, we can infer that comments in a subreddit that cover a broader scope and global events tend to cause more controversy than comments that focus on specific events."
      ],
      "metadata": {
        "id": "ZXwKlEdRfnAB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#c. Time Series"
      ],
      "metadata": {
        "id": "hJol0v4VnSMw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continuing with the controversiality feature, we can track the occurrence of controversial comments and compare its trends to real-world events. We will use a proportion of controversial posts rather than a count to obtain a more precise depiction of trends."
      ],
      "metadata": {
        "id": "ZmUXMeT88_tU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.dates as mdates\n",
        "\n",
        "df = pd.read_csv('sampled_data.csv')\n",
        "\n",
        "df['Date'] = pd.to_datetime(df['created_time'])\n",
        "controversial_posts = df[(df['controversiality'] >= 0.98) & (df['controversiality'] <= 1)]\n",
        "\n",
        "monthly_totals = df.resample('M', on='Date').size().reset_index(name='Total_Posts')\n",
        "monthly_controversial_counts = controversial_posts.resample('M', on='Date').size().reset_index(name='Controversial_Posts')\n",
        "\n",
        "monthly_data = pd.merge(monthly_totals, monthly_controversial_counts, on='Date', how='left')\n",
        "monthly_data['Proportion_Controversial'] = monthly_data['Controversial_Posts'] / monthly_data['Total_Posts']\n",
        "\n",
        "plt.figure(figsize=(20, 6))\n",
        "plt.plot(monthly_controversial_counts['Date'],monthly_data['Proportion_Controversial'],\n",
        "         marker='o', linestyle='-', color='crimson', label='Controversial Posts')\n",
        "\n",
        "plt.title('Monthly Proportion of Controversial Posts Over Time', fontsize=16)\n",
        "plt.xlabel('Month', fontsize=14)\n",
        "plt.ylabel('Proportion of Controversial Posts', fontsize=14)\n",
        "\n",
        "plt.gca().xaxis.set_major_locator(mdates.MonthLocator())\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "plt.legend(fontsize=12)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ogS1_7Lyq3nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A time series visualization was the chosen method to display such data as it can help us identify key trends over time and if such trends can relate to the major events of the Israel-Palestine War. If such correlations can be inferred, it may also be possible to predict future public opinion trends depending on real-time world events."
      ],
      "metadata": {
        "id": "BDAeMS_4FhYi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This graph shows the proportion of controversial comments compared to all comments each month from October 2023 to the present. The graph begins with a high proportion of controversial posts, likely due to the major event of the Israel-Palestine War that began on October 7, 2023, when the militant group Hamas launched a large-scale attack on Israel. Another noticeable spike appears in the data between December 2023 and January 2024. This may be related to South Africa initiating legal proceedings against Israel at the International Court of Justice on December 29, 2023, accusing Israel of committing genocide against Palestinians. Then, we can observe a decline in the proportion of controversial comments in June 2024. This decline may be attributed to the various global elections that took place around this time, which likely shifted worldwide attention away from the Israel-Palestine conflict. Finally, we observe an increase in the proportion of controversial comments as we approach October 2024 due to the assassination of Hamas leader Yahya Sinwar and the 1-year mark of the October 7 event. This uptick may reflect the continued global attention on the Israel-Palestine conflict. However, we also see a gradual decline in controversial posts around the time of the U.S. presidential elections, likely due to the shift in global focus toward the elections and related political events."
      ],
      "metadata": {
        "id": "icXckLzH84aP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MS_Q1ZeBsReV"
      },
      "source": [
        "##d. TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXH11n0BySKH"
      },
      "outputs": [],
      "source": [
        "document = [all_tokens]\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=list(all_stopwords))\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(document)\n",
        "terms = tfidf_vectorizer.get_feature_names_out()\n",
        "scores = tfidf_matrix.toarray()[0]\n",
        "tfidf_df = pd.DataFrame({'term': terms, 'tfidf_score': scores})\n",
        "tfidf_df = tfidf_df.sort_values(by='tfidf_score', ascending=False)\n",
        "tfidf_df.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6nj7_M43mWG"
      },
      "outputs": [],
      "source": [
        "df['joined_text'] = df['self_text_tokens'].apply(lambda tokens: \" \".join(tokens))\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(df['joined_text'])\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "results = []\n",
        "for idx, row in enumerate(tfidf_matrix.toarray()):\n",
        "    comment_scores = sorted(zip(terms, row), key=lambda x: x[1], reverse=True)\n",
        "    results.append(pd.DataFrame(comment_scores, columns=['term', 'tfidf_score']))\n",
        "\n",
        "for i, df in enumerate(results):\n",
        "    print(f\"\\nComment {i + 1} TF-IDF Scores:\")\n",
        "    print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF evaluates a term's significance in a document relative to a corpus. For this project, TF-IDF is applied to Reddit comments using tokenization, stopword removal, and scikit-learn's TfidfVectorizer. The top five terms for each comment are ranked by their TF-IDF scores to identify the most significant words of each comment. This analysis contributes to the study by providing insight into the unique views of each comment and how they compare to the comprehensive discussion regarding the topic. We will later be utilizing these TF-IDF scores for K-Means Clustering."
      ],
      "metadata": {
        "id": "6Pl8qcvX8EBA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XP3iPeTap40W"
      },
      "source": [
        "#**4. Methods, Techniques, and Algorithms**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**a. Sentiment Analysis**\n",
        "Sentiment analysis is a text analysis process pertaining to the connotation, or emotion, of a text. Here, we want to focus on the classification of each comment as negative, neutral, or positive, with values signifying the possibility of each. We will perform this via pre-trained sentiment analysis models on HuggingFace."
      ],
      "metadata": {
        "id": "jxdww-plxv2I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 1:\n",
        "[DistilBERT Base Uncased Sentiment Reddit Model](https://huggingface.co/mwkby/distilbert-base-uncased-sentiment-reddit-crypto)\n",
        "\n",
        "https://huggingface.co/blog/sentiment-analysis-python"
      ],
      "metadata": {
        "id": "A76fZtBKIjmw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4zWiXdk9Gqp"
      },
      "outputs": [],
      "source": [
        "model_name='mwkby/distilbert-base-uncased-sentiment-reddit-crypto'\n",
        "sentiment_task = pipeline(\"sentiment-analysis\", model=model_name, tokenizer=model_name)\n",
        "\n",
        "def get_sentiment(tokens, chunk_size=256):\n",
        "    if not tokens:\n",
        "        return \"NEUTRAL\"\n",
        "\n",
        "    sentiments = []\n",
        "    for i in range(0, len(tokens), chunk_size):\n",
        "        chunk = tokens[i:i + chunk_size]\n",
        "        text = \" \".join(chunk)\n",
        "        sentiment = sentiment_task(text)[0]['label']\n",
        "        sentiments.append(sentiment)\n",
        "\n",
        "    return max(set(sentiments), key=sentiments.count)\n",
        "\n",
        "df['sentiment'] = df['self_text_tokens'].apply(get_sentiment)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first model we attempted to implement was the DistilBERT Base Uncased Sentiment Reddit Model from HuggingFace. We thought this model would fit our data well due to the fact that it is trained on Reddit data. However, the model ended up not producing very accurate results, which we could tell just from looking at a sample of 10000 posts. For instance, it flagged “They are both beautiful with great food and good weather” as \"negative\" despite its overwhelmingly positive sentiment. We judge that this discrepacncy was born from the fact that we used our own tokenized column with our custom stopwords removed, which may have removed too much context and possibly terms important for analysis. Learning from this experience, we implemented a different model and different tokenization approach, which will be discussed in the next section."
      ],
      "metadata": {
        "id": "RMTMK9h3RmL0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 2:\n",
        "[Twitter roBERTa Base Sentiment Model](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest)"
      ],
      "metadata": {
        "id": "ItRqMbswIqFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "config = AutoConfig.from_pretrained(MODEL)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
        "\n",
        "def get_sentiment_scores(text, chunk_size=512):\n",
        "    sentiments = []\n",
        "    for i in range(0, len(text), chunk_size):\n",
        "        chunk = text[i:i + chunk_size]\n",
        "        encoded_input = tokenizer(chunk, return_tensors='pt', padding=True)\n",
        "        output = model(**encoded_input)\n",
        "        scores = output[0][0].detach().numpy()\n",
        "        scores = softmax(scores)\n",
        "        sentiments.append(scores)\n",
        "\n",
        "    average_sentiment = np.mean(sentiments, axis=0)  # Average the sentiment scores of each category for that comment\n",
        "    return average_sentiment\n",
        "\n",
        "df['sentiment_scores'] = df['self_text'].apply(get_sentiment_scores)\n",
        "df['negative'] = [score[0] for score in df['sentiment_scores']]\n",
        "df['neutral'] = [score[1] for score in df['sentiment_scores']]\n",
        "df['positive'] = [score[2] for score in df['sentiment_scores']]\n",
        "df"
      ],
      "metadata": {
        "id": "LVOGE7qBb_iY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('final_sentiment.csv')\n",
        "def get_sentiment_label(row):\n",
        "    scores = [row['negative'], row['neutral'], row['positive']]\n",
        "    labels = ['negative', 'neutral', 'positive']\n",
        "    max_index = np.argmax(scores)\n",
        "    return labels[max_index]\n",
        "\n",
        "df['sentiment_model2'] = df.apply(get_sentiment_label, axis=1)"
      ],
      "metadata": {
        "id": "OrV38HqPj4YJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model implementation we decided to use for the rest of our analysis is also from HuggingFace, the Twitter roBERTa Base Sentiment Model. Although this model was trained on Twitter data, we found it well-suited for analyzing Reddit comments as well, since both are similarly opinionated social media platforms with short text (Comments/Tweets). Using the model, we derive the sentiment scores of each post with three metrics, negative, neutral, and positive. Each of the three is assigned a value from 0-1, indicating the probability of that sentiment. The 3 values add up to a total of 1. We use the `get_sentiment_label` function to assign each comment negative, neutral, or positive based on the highest value recorded among the three categories.\n",
        "\n",
        "*Due to device limitations (lack of GPU) and Colab plan limitations, we had to restrict the size of our database for methods to run. We utilized a random sample of 10k comments, which takes approximately an hour to run."
      ],
      "metadata": {
        "id": "IWkuR1hRYMSZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizations:"
      ],
      "metadata": {
        "id": "sCGEzP7sJArp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subreddit_sentiments = df.groupby('subreddit')[['negative', 'neutral', 'positive']].mean()\n",
        "subreddit_sentiments = subreddit_sentiments.sort_values(by='negative', ascending=False)\n",
        "\n",
        "subreddits = subreddit_sentiments.index\n",
        "negative_scores = subreddit_sentiments['negative']\n",
        "neutral_scores = subreddit_sentiments['neutral']\n",
        "positive_scores = subreddit_sentiments['positive']\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "bar_width = 0.2\n",
        "\n",
        "ax.bar(x_pos - bar_width, negative_scores, width=bar_width, label='Negative')\n",
        "ax.bar(x_pos, neutral_scores, width=bar_width, label='Neutral')\n",
        "ax.bar(x_pos + bar_width, positive_scores, width=bar_width, label='Positive')\n",
        "\n",
        "ax.set_xticks(x_pos)\n",
        "ax.set_xticklabels(subreddits, rotation=45, ha='right')\n",
        "ax.set_xlabel('Subreddit')\n",
        "ax.set_ylabel('Average Sentiment Score')\n",
        "ax.set_title('Average Sentiment Scores by Subreddit')\n",
        "ax.legend()"
      ],
      "metadata": {
        "id": "dY6JHiUVuWup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This bar graph displays the average sentiment scores of each subreddit. There exists only one subreddit that does not have negative sentiment with a leading score, suggesting a majority of negative sentiment across all subreddits. The farthest left subreddit, r/NoahGetTheBoat, is focused around \"horrible acts from humanity,\" which makes sense as to why it is ranked highest in negative sentiment."
      ],
      "metadata": {
        "id": "0166ZCm5b97k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_counts = df['sentiment_model2'].value_counts()\n",
        "\n",
        "labels = sentiment_counts.index\n",
        "sizes = sentiment_counts.values\n",
        "\n",
        "plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)\n",
        "plt.title('Proportions of Sentiment Across Comments')"
      ],
      "metadata": {
        "id": "-lKoQs3FQGST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall sentiment surrounding the Israel-Palestine conflict tends to lean toward negative with 66.4% of the data being identified as negative. This can be attributed to the fact that the Israel-Palestine conflict is a highly sensitive topic with ongoing violence which is difficult to make poaitive comments about."
      ],
      "metadata": {
        "id": "sAsIJ8NmcIv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('final_sentiment (1).csv')"
      ],
      "metadata": {
        "id": "RNMHlKStZvL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sentiment_wordcloud(df, sentiment):\n",
        "    sentiment_wc = df[df['sentiment_model2'] == sentiment]\n",
        "    all_tokens = \" \".join([\" \".join(tokens) for tokens in sentiment_wc['self_text_tokens']])\n",
        "\n",
        "    wordcloud = WordCloud(\n",
        "        width=800,\n",
        "        height=400,\n",
        "        background_color='white',\n",
        "        colormap='magma',\n",
        "        normalize_plurals=False,\n",
        "        stopwords=all_stopwords,\n",
        "        max_words=100\n",
        "    ).generate(all_tokens)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Most Common Words in Comments with {sentiment} Sentiment\")\n",
        "    plt.show()\n",
        "\n",
        "generate_sentiment_wordcloud(df, 'positive')\n",
        "generate_sentiment_wordcloud(df, 'negative')\n",
        "generate_sentiment_wordcloud(df, 'neutral')"
      ],
      "metadata": {
        "id": "cYbc2NW_q6_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use word clouds to illustrate the general narratives of comments based on its sentiment. In our word cloud of positive comments, it is evident that there is a greater occurrence of lighthearted and optimistic terms such as \"great\", \"love\", \"good\", and \"peace\". In our word cloud of negative comments, there are much more sensitive terms associated with conflict such as \"war\", \"killed\", \"attack\", and \"genocide\". Lastly, in our word cloud of neutral comments, there exists a larger quantity of general informative language such as \"Israel\", \"US\", \"Hamas\", and \"government\". This helps us understand te general conversations that are relevant to each sentiment category."
      ],
      "metadata": {
        "id": "hyhxehgXhwAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_subreddits = ['IsraelPalestine', 'worldnews', 'Palestine']\n",
        "filtered_df = df[df['subreddit'].isin(top_subreddits)]\n",
        "\n",
        "filtered_df.loc[:, 'created_time'] = pd.to_datetime(filtered_df['created_time'])\n",
        "weekly_sentiment = filtered_df.groupby([pd.Grouper(key='timestamp', freq='2W-MON'), 'subreddit'])['negative'].mean().reset_index()\n",
        "\n",
        "sentiment_pivot = weekly_sentiment.pivot(index='timestamp', columns='subreddit', values='negative')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "for subreddit in top_subreddits:\n",
        "    ax.plot(sentiment_pivot.index, sentiment_pivot[subreddit], label=subreddit)\n",
        "\n",
        "ax.legend()\n",
        "ax.set_xlabel('Time')\n",
        "ax.set_ylabel('Negative Sentiment')\n",
        "ax.set_title('Negative Sentiment Trend for the Top 3 Subreddits Over Time')\n",
        "ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
        "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iuDK2Dwhs4Z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As mentioned previously, we can use a time series visualization to best depict trends in our data. Such trends can help determine associations with global events and predict future trends."
      ],
      "metadata": {
        "id": "0MUYTuWUFmuv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this time series visualization, we can see the negative sentiment trend for the top 3 subreddits over time. One thing to note is that throughout the entire time frame of this dataset, the subreddits “Palestine” and “IsraelPalestine” appear to have opposing trends with the subreddit “worldnews” in negative sentiment. This may indicate that, on average, the subreddits also had opposing views throughout the recorded time frame. We can identify a spike in negative sentiment near September 2024, which may be associated with the involvement of Lebanon and Iran in the Israel-Palestine War. Overall, the “worldnews” subreddit had less negative sentiment than the other two subreddits."
      ],
      "metadata": {
        "id": "OL5yJYJS73EW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weekly_sentiment = filtered_df.groupby([pd.Grouper(key='created_time', freq='2W-MON'), 'subreddit'])['positive'].mean().reset_index()\n",
        "sentiment_pivot = weekly_sentiment.pivot(index='created_time', columns='subreddit', values='positive')\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "for subreddit in top_subreddits:\n",
        "    ax.plot(sentiment_pivot.index, sentiment_pivot[subreddit], label=subreddit)\n",
        "\n",
        "ax.set_ylim(0, 0.66) # Setting the y-axis similarly to the negative plot for comparison\n",
        "\n",
        "ax.set_xlabel('Time')\n",
        "ax.set_ylabel('Negative Sentiment')\n",
        "ax.set_title('Positive Sentiment Trend for the Top 3 Subreddits Over Time')\n",
        "ax.legend()\n",
        "ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
        "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "1kVJAJ0btq6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this time series visualization, we can see the positive sentiment trend for the top 3 subreddits over time. As we saw in the pie chart, the positive sentiment score of comments is overall much lower than the negative sentiment scores in the previous graph. Additionally, like the previous visualization, we can identify opposing trends in positive sentiment between the subreddits “Palestine” and “IsraelPalestine”. Another intriguing factor of this visualization is that the “worldnews” subreddit has the tendency to be in between the data of the other subreddits. This may indicate less bias as the subreddit likely does not show much positive sentiment towards either side. Thus, we can infer that the “worldnews” subreddit may have a more neutral stance amongst the top subreddits."
      ],
      "metadata": {
        "id": "PWluLnT-75kD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All in all, our implementation of sentiment analysis provided deeper context to the public discourse surrounding this war. From our visualizations, we can conclude that the majority of such public discourse is negative, with varying spikes and drops indicating reactions to global events. We can also claim that differing trends in sentiment exist across the subreddits, which may help determine the nuances in neutrality and bias."
      ],
      "metadata": {
        "id": "yjgKHkMz_c6Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **b. K Means Clustering**"
      ],
      "metadata": {
        "id": "p4wBDF0ara_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "K Means Clustering is a unsupervised learning technique that groups similar data points together. Since it is unsupervised, it does not need the data to have pre-exisiting labels. We are using it in our project to help group our tokens into different clusters so that we can find the main topics that are being discussed about the conflict on Reddit."
      ],
      "metadata": {
        "id": "nwebni8kGOps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('final.csv')"
      ],
      "metadata": {
        "id": "tUCqiiIoJ6VE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original TF-IDF Shape:\", tfidf_matrix.shape)\n",
        "\n",
        "svd = TruncatedSVD(n_components=2, random_state=42)\n",
        "data_2d = svd.fit_transform(tfidf_matrix)\n",
        "print(\"Reduced TF-IDF Shape:\", data_2d.shape)\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaled_tfidf_matrix = scaler.fit_transform(data_2d)"
      ],
      "metadata": {
        "id": "L8Id446BQYW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we reduced the dimensionality of our TF-IDF array from 19470 down to 2. This is because K Means Clustering does not do well with high dimensions so this is the first thing we need to do before continuing on."
      ],
      "metadata": {
        "id": "fcfcWTq4mjZD"
      }
    },
    {
      "source": [
        "cluster_range = range(1, 10)\n",
        "\n",
        "wcss = []\n",
        "\n",
        "for num_clusters in cluster_range:\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "    kmeans.fit(scaled_tfidf_matrix)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "plt.plot(cluster_range, wcss, marker='o')\n",
        "plt.title('Elbow Method for Optimal k')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Within-Cluster Sum of Squares (WCSS)')\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "lQcy4z2Zr1Vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "num_clusters = 5\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "kmeans.fit(data_2d)\n",
        "\n",
        "df['cluster'] = kmeans.labels_\n",
        "\n",
        "silhouette_avg = silhouette_score(data_2d, df['cluster'])\n",
        "print(f\"Silhouette Score: {silhouette_avg}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "rPTjpbJYvbwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We utilized two different methods for helping choose our k value for k means clustering. First we created a graph that visualizes the WCSS values for the different number of clusters. In this graph we can see there is an inflection point at k = 3 and a much smaller one at k = 5. Then we checked the silhouette score for both and ended up getting a higher score on k = 3. However, even though k = 5 has a lower silhouette score, we were able to find more insightful topics that were discussed with 5 clusters instead of 3 so we chose k = 5.   "
      ],
      "metadata": {
        "id": "u83AQZ48mrPx"
      }
    },
    {
      "source": [
        "df['cluster'] = df['cluster'].astype('category')\n",
        "scatter = plt.scatter(data_2d[:, 0], data_2d[:, 1], c=df['cluster'], cmap=plt.cm.get_cmap('viridis', len(df['cluster'].unique())))\n",
        "plt.title(\"Cluster Visualization\")\n",
        "plt.colorbar(scatter, label='Cluster')\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "KJaPd-aqXdBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we reduced our dimensions down to 2, we can create a visualization like this that displays like a map for our clusters. Each unique color is a different cluster and we can see some clusters are well defined and packed in while other clusters are more loose and contain outliers in our data. The more loose clusters could represent more nuanced discussions with multiple sub-topics or it could show the cluster's topic is not as well defined."
      ],
      "metadata": {
        "id": "19q8HUTQmvN1"
      }
    },
    {
      "source": [
        "for cluster_num in range(kmeans.n_clusters):\n",
        "    cluster_docs = tfidf_matrix[df['cluster'] == cluster_num]\n",
        "    mean_tfidf = cluster_docs.mean(axis=0).A1\n",
        "\n",
        "    term_weights = dict(zip(terms, mean_tfidf))\n",
        "    term_weights_lower = {k.lower(): v for k, v in term_weights.items()}\n",
        "\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='plasma',\n",
        "      stopwords=all_stopwords).generate_from_frequencies(term_weights_lower)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Word Cloud for Cluster {cluster_num + 1}\")\n",
        "    plt.show()\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Hz11DVhjnKX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_terms = {}\n",
        "for i in range(kmeans.n_clusters):\n",
        "    cluster_terms[i] = tfidf_matrix[df['cluster'] == i]\n",
        "\n",
        "for cluster_num in range(kmeans.n_clusters):\n",
        "    print(f\"\\nCluster {cluster_num + 1} Top Terms:\")\n",
        "\n",
        "    cluster_docs = tfidf_matrix[df['cluster'] == cluster_num]\n",
        "\n",
        "    mean_tfidf = cluster_docs.mean(axis=0).A1\n",
        "\n",
        "    terms_df = pd.DataFrame({'term': terms, 'score': mean_tfidf})\n",
        "    top_terms = terms_df.sort_values(by='score', ascending=False).head(20)\n",
        "\n",
        "    print(top_terms)\n"
      ],
      "metadata": {
        "id": "YwLirJq3nLQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the results of the word clouds and the top 20 TF-IDF terms in each cluster, we can get a better understanding of the overall topic the clusters separated into.\n",
        "\n",
        "##*Cluster 1:*\n",
        "\n",
        "This cluster’s top terms have a mix of positive and negative sentiment words, such as \"good\", \"well\", \"thank\", \"bad\". However, there are mostly neutral general words like \"work\", \"way\", \"thing\". This suggests the cluster is focusing on the general discussions and quick responses people have about the conflict, while also talking about practical implications and effects.\n",
        "\n",
        "\n",
        "\n",
        "##*Cluster 2:*\n",
        "\n",
        "Cluster 2 becomes significantly more specific as it is centered around the military conflict, especially in relation to Gaza. Words such as \"military\", \"attack\", \"rockets\", \"killed\", \"children\" depict the grim reality of those in the area. We can establish that this cluster is heavily focused on military actions and the impacts they have on the civilians of Gaza.\n",
        "\n",
        "\n",
        "\n",
        "##*Cluster 3:*\n",
        "\n",
        "Within cluster 3, we see a more political, historical, and ideological discussion of the conflict. \"Zionism\", \"antisemitism\", \"jewish\", and \"peace\", help show historical motivations as well as global implications of the war. Overall, the cluster tries to focus in on the complex nature of the Israel-Palestine issue\n",
        "\n",
        "\n",
        "##*Cluster 4:*\n",
        "\n",
        "This cluster seems to focus on the discussion of the United States and its role in respect to the conflict. Words like \"us\", \"country\", \"american\", \"government\", \"support\", \"world\", \"side\" suggest public opinion debating on the US involvement along with other countries in the world. This cluster may relate to the strong alliance between the United States and Israel.\n",
        "\n",
        "\n",
        "\n",
        "##*Cluster 5:*\n",
        "\n",
        "For the last cluster, the topic is centered around support for the Hamas and Palestinian side while talking about the cruelty of Israeli policies. This is seen with words such as \"Hamas\", \"terrorist\", \"innocent\", \"ceasefire\", \"pro\", \"right\", \"stop\". It is evident that this cluster deals with controversial opinions in regards to the war while showing public opinion in favor of Hamas and Palestine.\n",
        "\n",
        "\n",
        "\n",
        "There are many nuanced topics about the Israel-Palestine conflict that can be seen with these different clusters. Even though we can see certain tokens repeated across different clusters, the topic surrounding each token varies depending on the context of the cluster, as seen through the TF-IDF analysis. For example, \"Israel\" is discussed in context of Hamas and the Palestinians for one cluster, Jewish identity and statehood in another, while also in context of US involvement and the morality of the war. By examining these clusters, we gain a deeper understanding of the different layers of the Israel-Palestine conflict as reflected in public discourse. Such diverse layers highlight the complexity of the topic and the narratives that exist within ongoing conversations."
      ],
      "metadata": {
        "id": "XBxqF5M9kTJq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**c. N-Grams**"
      ],
      "metadata": {
        "id": "OC5BCle57mf8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "N-Grams is a technique used for text analysis that groups the tokens in our dataset into N sized groups. For example if N is 2, then it would group the tokens into pairs. With the tokens grouped this way, we can find the most frequent N-Grams to provide more context on the public opinion than just looking at one token at a time. Here we will be using Bigrams and Trigrams, which will be looking at pairs and triplets."
      ],
      "metadata": {
        "id": "4vz8FohmG8k4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "\n",
        "def generate_ngrams(text_list, n):\n",
        "    filtered_text_list = [word for word in text_list if len(word) > 2]\n",
        "    return list(ngrams(filtered_text_list, n))\n",
        "\n",
        "all_bigrams = []\n",
        "all_trigrams = []\n",
        "\n",
        "for tokens in df['self_text_tokens']:\n",
        "    all_bigrams.extend(generate_ngrams(tokens, 2))\n",
        "    all_trigrams.extend(generate_ngrams(tokens, 3))\n",
        "\n",
        "bigram_counts = Counter(all_bigrams)\n",
        "\n",
        "top_bigrams = bigram_counts.most_common(10)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.barh([f\"{w1} {w2}\" for w1, w2 in dict(top_bigrams).keys()], dict(top_bigrams).values(), color='skyblue')\n",
        "plt.xlabel(\"Frequency\")\n",
        "plt.title(\"Top 10 Bigrams\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AUKTLXb50-M1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the result of our Bigrams analysis. The top bigrams seem to just be about the geographical location of the conflict. \"Ethnic cleansing\" is related to what people think the conflict is stemming from which is still widely debated on. \"October 7th\" is the date of the major event where Hamas attacked Isreal. \"State solution\" shows the discussion on a possible solution to the conflict of having Israelis and Palestinians have their own independent state. Lastly for \"palestinian people\", it shows the discussion is mostly in the perspective of Palestinians."
      ],
      "metadata": {
        "id": "-cS_DpAP9t4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trigram_counts = Counter(all_trigrams)\n",
        "\n",
        "top_trigrams = trigram_counts.most_common(10)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.barh([f\"{w1} {w2} {w3}\" for w1, w2, w3 in dict(top_trigrams).keys()], dict(top_trigrams).values(), color='salmon')\n",
        "plt.xlabel(\"Frequency\")\n",
        "plt.title(\"Top 10 Trigrams\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PiTTpGQ13Q6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this N-Grams graph, we increased the size to be trigrams. The most frequent trigrams are again just about the geography of the conflict. \"Israel right exist\" is a bit ambigious but we can assume it means Israelis have the right to exist. This could signify public opinion backing Israel. Other trigrams such as \"Hamas terrorist organization\", \"using human shields\", \"Hamas started war\" also lean towards that idea as well. \"Second class citizens\" relates towards how Palestinians are treated like second class citizens which sparks discussions about how Palestinians are being discriminated against and not given the same rights. Overall, the top trigrams shown in the graph portray that their is support and discussion for both sides of the conflict. With the bigrams there can be seen a lean in support towards the Palestinians while for trigrams there can be seen a lean in support towards the Israeli side."
      ],
      "metadata": {
        "id": "u1q_lL9c_QZB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n"
      ],
      "metadata": {
        "id": "epB67-Slaagr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With a comprehensive analysis of this data, we can gain a deeper understanding of the various elements of the Israel-Palestine War as presented in public discourse. This study holds great significance considering the war is an ongoing conflict with geopolitical, social, and humanitarian implications. As displayed in a number of our visualizations, the top two leading terms of this study are “Israel” and “Hamas”. Therefore, it can be inferred that the public views the conflict as more of a war between Israel and Hamas rather than between Israel and Palestine. This suggests the idea that people perceive Hamas as the leading representative of all Palestinian people.\n",
        "\n",
        "Demonstrated by assorted time series visualizations, we can associate certain trends and changes in our data with major events relevant to the Israel-Palestine War. We can associate significant changes in occurrence of controversiality with significant geopolitical events. For example, the assassination of Hamas leader, Yahya Sinwar (October 2024), is consistent with spikes in controversiality, while the involvement of Lebanon and Iran led to spikes in negative sentiment. Adding on, the sentiment analysis of individual subreddits established that there exists subreddits with less bias and more neutrality than others. Overall, the majority of the dataset was identified as “negative”, which can be established to be in accordance with the sensitive nature of the topic.\n",
        "\n",
        "In addition, the topics discovered in the K-Means Clustering showed that some of the main topics were in support of Hamas and Palestine along with international support for them as well. Many of the most frequent bigrams also display support for Palestinians and discuss possible solutions to the conflict. In comparison, the trigrams present more advocacy for Israel and criticize the actions of Hamas.\n",
        "\n",
        "Our analysis is currently constrained mainly by the dataset size (lack of access to stronger processing units) and limitations of sentiment models; however, a vision we have for our project's next step could be predicting responses to real-world events. Specifically, we hope to train our own machine learning model to identify hate speech or misleading content early on to improve current measures set in place to remove/prevent such content.\n"
      ],
      "metadata": {
        "id": "VhsrDUuDdU2D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Contribution:\n",
        "Risa Onishi: Sentiment analysis implementation and all corresponding visualizations. Assisted in data cleaning/tokenization. Report organization and finalization. Final presentation video editing.\n",
        "\n",
        "Ashna Pradhan: Created and analyzed word cloud and time series in the EDA. Conducted TF-IDF calculations on all tokens. Provided analysis of geopolitical events.\n",
        "\n",
        "Sazen Shakya: Implemented K Means Clustering with word cloud and scatter plot visualizations. Conducted N-Grams analysis on bigrams and trigrams and created respective bar graph to display.\n",
        "\n",
        "Haru Sakamoto: Worked on the text processing, including data cleaning and tokenization. Created bar graphs on the subreddits in the EDA and added descriptions. Assisted with sentiment analysis visualizations.  \n",
        "\n",
        "\n",
        "\n",
        "|\n"
      ],
      "metadata": {
        "id": "14v60eNQ4Hg2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Citations:\n",
        "https://acleddata.com/middle-east/regional-overviews/\n"
      ],
      "metadata": {
        "id": "Q8FrkYKK3T8Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "|\n",
        "\n",
        "|\n",
        "\n",
        "|\n",
        "\n",
        "|\n",
        "\n",
        "|\n",
        "\n",
        "|\n",
        "\n",
        "|\n",
        "\n",
        "|\n",
        "|\n",
        "\n",
        "|\n",
        "\n",
        "|\n",
        "\n",
        "|\n",
        "|\n",
        "\n",
        "|\n",
        "\n",
        "|\n",
        "\n",
        "|\n",
        "\n",
        "|\n",
        "\n",
        "|\n",
        "\n",
        "|\n",
        "\n",
        "|\n",
        "\n",
        "|\n",
        "\n",
        "|\n",
        "\n",
        "|\n",
        "\n",
        "|\n",
        "\n",
        "|\n",
        "\n",
        "|\n",
        "\n",
        "|\n",
        "\n",
        "|\n",
        "\n",
        "|\n",
        "\n",
        "|\n",
        "\n",
        "|\n",
        "\n",
        "|\n",
        "\n",
        "|\n",
        "\n",
        "|\n",
        "\n",
        "|\n",
        "\n",
        "|"
      ],
      "metadata": {
        "id": "-SpNN1jt3abb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e4On9wtL3ich"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "|\n",
        "\n",
        "|\n",
        "\n",
        "|\n",
        "\n",
        "|\n",
        "\n",
        "|\n",
        "\n",
        "|\n",
        "\n",
        "|\n",
        "\n",
        "|\n",
        "\n",
        "|\n",
        "\n",
        "|\n",
        "\n",
        "|\n",
        "\n",
        "|\n",
        "\n",
        "|\n",
        "\n",
        "|\n",
        "\n",
        "|\n",
        "\n",
        "|\n"
      ],
      "metadata": {
        "id": "YdZQPXWe3izs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kjhmy5gr3j34"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}